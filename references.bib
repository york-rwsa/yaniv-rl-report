
@book{levinson_chance_2001,
	title = {Chance, luck, and statistics},
	publisher = {Courier Corporation},
	author = {Levinson, Horace C},
	year = {2001},
}

@book{david_sidney_parlett_oxford_1990,
	title = {The {Oxford} guide to card games},
	isbn = {978-0-19-214165-1},
	url = {http://archive.org/details/oxfordguidetocar00parl},
	language = {eng},
	urldate = {2021-05-13},
	publisher = {Oxford University Press},
	author = {{David Sidney Parlett}},
	collaborator = {{Internet Archive}},
	year = {1990},
	keywords = {Card games -- History},
}

@inproceedings{eswaran_game_2020,
	address = {Virtual Event CA USA},
	title = {Game {Action} {Modeling} for {Fine} {Grained} {Analyses} of {Player} {Behavior} in {Multi}-player {Card} {Games} ({Rummy} as {Case} {Study})},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403316},
	doi = {10.1145/3394486.3403316},
	abstract = {We present a deep learning framework for game action modeling, which enables fine-grained analyses of player behavior. We develop CNN-based supervised models that effectively learn the critical game play decisions from skilled players, and use these models to assess player characteristics in the system, such as their retention, engagement, deposit buckets, etc. We show that with a carefully constructed input format, that efficiently represents the game state and history as a multi-dimensional image, along with a custom architecture the model learns the strategies of the game accurately. It is further enhanced with look-ahead achieved by self-play simulation to better estimate the game state, and this information is used in a new loss function. Next, we show that analyzing the players with these models as reference has immense benefit in understanding player potential in terms of engagement and revenue. We also use the model to understand the various contexts under which players tend to make mistakes, and use these insights to up-skill players.},
	language = {en},
	urldate = {2021-05-13},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Eswaran, Sharanya and Sachdeva, Mridul and Vimal, Vikram and Seth, Deepanshi and Kalpam, Suhaas and Agarwal, Sanjay and Mukherjee, Tridib and Dattagupta, Samrat},
	month = aug,
	year = {2020},
	pages = {2657--2665},
}

@inproceedings{akramizadeh_different_2009,
	title = {Different forms of the games in multiagent reinforcement learning: alternating vs. simultanous movements},
	shorttitle = {Different forms of the games in multiagent reinforcement learning},
	doi = {10.1109/MED.2009.5164724},
	abstract = {Multiagent systems are one of the most promising solutions in most of real life applications in which some kinds of social interactions or conventions are involved. Agent oriented applications are broadly explored among which learning in unknown environment is well developed based on Markov Decision Process (MDP). On the other hand, learning in multiagent systems has been recently introduced, basically in conjunction with game theory which is the science of investigating multiple interactive agents. During learning, self-interested agents are attempting to find the equilibrium policy based on the structure of the game, mostly considered as normal form games. In this paper, we focus on bringing into discussion game structures, addressed as normal form games and extensive form games, in learning process. This includes also some modifications and refinements in initially introduced concepts as well as a proposed approach in extensive form games.},
	booktitle = {2009 17th {Mediterranean} {Conference} on {Control} and {Automation}},
	author = {Akramizadeh, Ali and Afshar, Ahmad and Menhaj, Mohammad-B},
	month = jun,
	year = {2009},
	keywords = {Automatic control, Automation, Computational intelligence, Convergence, Decision making, Game theory, Large-scale systems, Learning, Multiagent reinforcement learning, Multiagent systems, Nash equilibrium, Nash equilibrium points, extensive form game, normal form game, subgame perfect equilibrium points},
	pages = {1289--1294},
}

@book{oliehoek_concise_2016,
	address = {Cham},
	series = {{SpringerBriefs} in {Intelligent} {Systems}},
	title = {A {Concise} {Introduction} to {Decentralized} {POMDPs}},
	isbn = {978-3-319-28927-4 978-3-319-28929-8},
	url = {http://link.springer.com/10.1007/978-3-319-28929-8},
	language = {en},
	urldate = {2021-05-12},
	publisher = {Springer International Publishing},
	author = {Oliehoek, Frans A. and Amato, Christopher},
	year = {2016},
	doi = {10.1007/978-3-319-28929-8},
}

@article{shoham_multiagent_nodate,
	title = {Multiagent {Systems}: {Algorithmic}, {Game}-{Theoretic}, and {Logical} {Foundations}},
	language = {en},
	author = {Shoham, Yoav},
	pages = {532},
}

@book{osborne_course_1994,
	address = {Cambridge, Mass},
	title = {A course in game theory},
	isbn = {978-0-262-15041-5 978-0-262-65040-3},
	language = {en},
	publisher = {MIT Press},
	author = {Osborne, Martin J. and Rubinstein, Ariel},
	year = {1994},
	keywords = {Game theory},
}

@misc{noauthor_concise_nodate,
	title = {A {Concise} {Introduction} to {Decentralized} {POMDPs} {\textbar} {Frans} {A}. {Oliehoek} {\textbar} {Springer}},
	url = {https://www.springer.com/gp/book/9783319289274},
	urldate = {2021-05-12},
}

@inproceedings{oliehoek_decentralized_2012,
	title = {Decentralized {POMDPs}},
	abstract = {Abstract This chapter presents an overview of the decentralized POMDP (Dec-POMDP) framework. In a Dec-POMDP, a team of agents collaborates to maximize a global reward based on local information only. This means that agents do not observe a Markovian signal during execution and therefore the agents ’ individual policies map from histories to actions. Searching for an optimal joint policy is an extremely hard problem: it is NEXP-complete. This suggests, assuming NEXP=EXP, that any optimal solution method will require doubly exponential time in the worst case. This chapter focuses on planning for Dec-POMDPs over a finite horizon. It covers the forward heuristic search approach to solving Dec-POMDPs, as well as the backward dynamic programming approach. Also, it discusses how these relate to the optimal Q-value function of a Dec-POMDP. Finally, it provides pointers to other solution methods and further related topics. 1},
	booktitle = {Reinforcement {Learning}: {State} of the {Art}},
	publisher = {Springer},
	author = {Oliehoek, Frans A.},
	year = {2012},
}

@incollection{littman_markov_1994,
	address = {San Francisco (CA)},
	title = {Markov games as a framework for multi-agent reinforcement learning},
	isbn = {978-1-55860-335-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
	abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
	language = {en},
	urldate = {2021-05-12},
	booktitle = {Machine {Learning} {Proceedings} 1994},
	publisher = {Morgan Kaufmann},
	author = {Littman, Michael L.},
	editor = {Cohen, William W. and Hirsh, Haym},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-1-55860-335-6.50027-1},
	pages = {157--163},
}

@article{bellman_markovian_1957,
	title = {A {Markovian} {Decision} {Process}},
	volume = {6},
	issn = {00959057, 19435274},
	url = {http://www.jstor.org/stable/24900506},
	number = {5},
	urldate = {2021-05-11},
	journal = {Journal of Mathematics and Mechanics},
	author = {BELLMAN, RICHARD},
	year = {1957},
	note = {Publisher: Indiana University Mathematics Department},
	pages = {679--684},
}

@book{silver_lectures_2015,
	title = {Lectures on {Reinforcement} {Learning}},
	author = {Silver, David},
	year = {2015},
	note = {Published: \textsc{url:} https://www.davidsilver.uk/teaching/},
}

@inproceedings{sutton_policy_2000,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	abstract = {Function approximation is essential to reinforcement learning, but  the standard approach of approximating a value function and determining  a policy from it has so far proven theoretically intractable. In this paper},
	booktitle = {In {Advances} in {Neural} {Information} {Processing} {Systems} 12},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year = {2000},
	pages = {1057--1063},
}

@book{puterman_markov_2014,
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	isbn = {978-1-118-62587-3},
	shorttitle = {Markov {Decision} {Processes}},
	abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. "This text is unique in bringing together so many results hitherto found only in part in other texts and papers. . . . The text is fairly self-contained, inclusive of some basic mathematical results needed, and provides a rich diet of examples, applications, and exercises. The bibliographical material at the end of each chapter is excellent, not only from a historical perspective, but because it is valuable for researchers in acquiring a good perspective of the MDP research potential." —Zentralblatt fur Mathematik ". . . it is of great value to advanced-level students, researchers, and professional practitioners of this field to have now a complete volume (with more than 600 pages) devoted to this topic. . . . Markov Decision Processes: Discrete Stochastic Dynamic Programming represents an up-to-date, unified, and rigorous treatment of theoretical and computational aspects of discrete-time Markov decision processes." —Journal of the American Statistical Association},
	language = {de},
	publisher = {John Wiley \& Sons},
	author = {Puterman, Martin L.},
	month = aug,
	year = {2014},
	note = {Google-Books-ID: VvBjBAAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{hazeghi_markov_1995,
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	volume = {90},
	issn = {01621459},
	shorttitle = {Markov {Decision} {Processes}},
	url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=01621459&v=2.1&it=r&id=GALE%7CA16679132&sid=googleScholar&linkaccess=abs},
	language = {English},
	number = {429},
	urldate = {2021-05-11},
	journal = {Journal of the American Statistical Association},
	author = {Hazeghi, Kasra},
	month = mar,
	year = {1995},
	note = {Publisher: American Statistical Association},
	pages = {392--394},
}

@article{arulkumaran_brief_2017,
	title = {A {Brief} {Survey} of {Deep} {Reinforcement} {Learning}},
	volume = {34},
	issn = {1053-5888},
	url = {http://arxiv.org/abs/1708.05866},
	doi = {10.1109/MSP.2017.2743240},
	abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
	number = {6},
	urldate = {2021-05-10},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.05866},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {26--38},
}

@article{hessel_muesli_2021,
	title = {Muesli: {Combining} {Improvements} in {Policy} {Optimization}},
	shorttitle = {Muesli},
	url = {http://arxiv.org/abs/2104.06159},
	abstract = {We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.},
	urldate = {2021-05-03},
	journal = {arXiv:2104.06159 [cs]},
	author = {Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez, Arthur and Schmitt, Simon and Sifre, Laurent and Weber, Theophane and Silver, David and van Hasselt, Hado},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.06159},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{guo_distinguishing_2019,
	title = {Distinguishing luck from skill through statistical simulation: a case study},
	issn = {0361-0918, 1532-4141},
	shorttitle = {Distinguishing luck from skill through statistical simulation},
	url = {https://www.tandfonline.com/doi/full/10.1080/03610918.2019.1698746},
	doi = {10.1080/03610918.2019.1698746},
	abstract = {To investigate the perential question of how to measure luck versus skill, we perform a detailed simulation study of Texas Hold’em poker hands. We deﬁne luck and skill as the player equity changes from dealing cards and from player betting decisions, respectively. We ﬁnd that a careful deﬁnition of player equity leads to measurements of luck and skill which satisfy the statistical properties that we should expect them to. We conclude that our deﬁnitions of luck versus skill appear to be valid and accurate in the context of poker hands, and perhaps beyond to issues of luck and skill in other aspects of modern society.},
	language = {en},
	urldate = {2021-05-01},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Guo, Zhaoyu and Khuu, Irwin and Zhu, Kevin and Rosenthal, Jeffrey S. and Schoenberg, Frederic P.},
	month = dec,
	year = {2019},
	pages = {1--23},
}

@article{li_deep_2018,
	title = {Deep {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1810.06339},
	abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
	urldate = {2021-04-29},
	journal = {arXiv:1810.06339 [cs, stat]},
	author = {Li, Yuxi},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.06339},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fuchs_theory_2021,
	title = {Theory of {Mind} for {Deep} {Reinforcement} {Learning} in {Hanabi}},
	url = {http://arxiv.org/abs/2101.09328},
	abstract = {The partially observable card game Hanabi has recently been proposed as a new AI challenge problem due to its dependence on implicit communication conventions and apparent necessity of theory of mind reasoning for efficient play. In this work, we propose a mechanism for imbuing Reinforcement Learning agents with a theory of mind to discover efficient cooperative strategies in Hanabi. The primary contributions of this work are threefold: First, a formal definition of a computationally tractable mechanism for computing hand probabilities in Hanabi. Second, an extension to conventional Deep Reinforcement Learning that introduces reasoning over finitely nested theory of mind belief hierarchies. Finally, an intrinsic reward mechanism enabled by theory of mind that incentivizes agents to share strategically relevant private knowledge with their teammates. We demonstrate the utility of our algorithm against Rainbow, a state-of-the-art Reinforcement Learning agent.},
	urldate = {2021-04-28},
	journal = {arXiv:2101.09328 [cs]},
	author = {Fuchs, Andrew and Walton, Michael and Chadwick, Theresa and Lange, Doug},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09328},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2021-04-27},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{liang_rllib_2018,
	title = {{RLlib}: {Abstractions} for {Distributed} {Reinforcement} {Learning}},
	shorttitle = {{RLlib}},
	url = {http://arxiv.org/abs/1712.09381},
	abstract = {Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available at https://rllib.io/.},
	urldate = {2021-04-27},
	journal = {arXiv:1712.09381 [cs]},
	author = {Liang, Eric and Liaw, Richard and Moritz, Philipp and Nishihara, Robert and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph E. and Jordan, Michael I. and Stoica, Ion},
	month = jun,
	year = {2018},
	note = {arXiv: 1712.09381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@inproceedings{jiang_deltadou_2019,
	address = {Macao, China},
	title = {{DeltaDou}: {Expert}-level {Doudizhu} {AI} through {Self}-play},
	isbn = {978-0-9992411-4-1},
	shorttitle = {{DeltaDou}},
	url = {https://www.ijcai.org/proceedings/2019/176},
	doi = {10.24963/ijcai.2019/176},
	abstract = {Artiﬁcial Intelligence has seen several breakthroughs in two-player perfect information game. Nevertheless, Doudizhu, a three-player imperfect information game, is still quite challenging. In this paper, we present a Doudizhu AI by applying deep reinforcement learning from games of self-play. The algorithm combines an asymmetric MCTS on nodes representing each player’s information set, a policy-value network that approximates the policy and value on each decision node, and inference on unobserved hands of other players by given policy. Our results show that self-play can signiﬁcantly improve the performance of our agent in this multiagent imperfect information game. Even starting with a weak AI, our agent can achieve human expert level after days of self-play and training.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Jiang, Qiqi and Li, Kuangzheng and Du, Boyao and Chen, Hao and Fang, Hai},
	month = aug,
	year = {2019},
	pages = {1265--1271},
}

@inproceedings{baykal_reinforcement_2019,
	title = {Reinforcement {Learning} in {Card} {Game} {Environments} {Using} {Monte} {Carlo} {Methods} and {Artificial} {Neural} {Networks}},
	doi = {10.1109/UBMK.2019.8907235},
	abstract = {Artificial intelligence has wide range of application areas and games are one of the important ones. There are many applications of artificial intelligence methods in game environments. It is very common for game environments to include intelligent agents. Having intelligent agents makes a game more entertaining and challenging for its players. Reinforcement learning methods can be applied to develop artificial intelligence agents that learn to play a game by themselves without any supervision and can play it at a high level of expertize. Supervised learning methods, on the other hand, can be applied to develop agents that play a game by imitating the supervisor players. The purpose of this study is to develop self-learning agents for a card game, namely Batak, using reinforcement learning combined with supervised learning. Batak is a trick-taking card game popular in Turkey. Results of the study reveal that the developed agents are better at gameplaying and similar at bidding compared to some rule based Batak playing agents.},
	booktitle = {2019 4th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Baykal, O. and Alpaslan, F. N.},
	month = sep,
	year = {2019},
	keywords = {artificial intelligence, games, learning systems, monte carlo methods, neural networks, reinforcement learning, state-value functions, supervised learning},
	pages = {1--6},
}

@article{wagenaar_learning_nodate,
	title = {Learning to play the {Game} of {Hearts} using {Reinforcement} {Learning} and a {Multi}-{Layer} {Perceptron}},
	abstract = {The multi-agent card game Hearts is learned by the use of Monte Carlo learning and a Multilayer Perceptron. This card game has imperfect state information and is therefore harder to learn than perfect information games. A few diﬀerent parameters will be looked at to attempt to ﬁnd out which combination is most promising. Most importantly, two activation functions, namely Sigmoid and a Leaky version of the Rectiﬁed Linear Unit (ReLU), will be compared and a variation in the amount of hidden layers will be studied. After experimentation it is concluded that the Sigmoid function is outperformed by the ReLU. Multiple hidden layers seem to slow the learning process down and do not improve performance.},
	language = {en},
	author = {Wagenaar, Maxiem and Wiering, Dr M},
	pages = {10},
}

@article{demirdover_hearts_nodate,
	title = {Hearts thesis},
	abstract = {Artiﬁcial intelligence and machine learning are widely popular in many sectors. One of them is the gaming industry. With many different scenarios, different types, games are perfect for machine learning and artiﬁcial intelligence. This study aims to develop learning agents to play the game of Hearts. Hearts is one of the most popular card games in the world. It is a trick based, imperfect information card game. In addition to having a huge state space, hearts offers many extra challenges due to the nature of the game. These challenges are divided into smaller parts where learning is easier and assigned to different learning agents. These agents use temporal difference learning to learn assigned parts.},
	language = {en},
	author = {Demirdöver, Ra Kaan},
	pages = {74},
}

@article{shi_scrofazero_2021,
	title = {{ScrofaZero}: {Mastering} {Trick}-taking {Poker} {Game} {Gongzhu} by {Deep} {Reinforcement} {Learning}},
	shorttitle = {{ScrofaZero}},
	url = {http://arxiv.org/abs/2102.07495},
	abstract = {People have made remarkable progress in game AIs, especially in domain of perfect information game. However, trick-taking poker game, as a popular form of imperfect information game, has been regarded as a challenge for a long time. Since trick-taking game requires high level of not only reasoning, but also inference to excel, it can be a new milestone for imperfect information game AI. We study Gongzhu, a trick-taking game analogous to, but slightly simpler than contract bridge. Nonetheless, the strategies of Gongzhu are complex enough for both human and computer players. We train a strong Gongzhu AI ScrofaZero from {\textbackslash}textit\{tabula rasa\} by deep reinforcement learning, while few previous efforts on solving trick-taking poker game utilize the representation power of neural networks. Also, we introduce new techniques for imperfect information game including stratified sampling, importance weighting, integral over equivalent class, Bayesian inference, etc. Our AI can achieve human expert level performance. The methodologies in building our program can be easily transferred into a wide range of trick-taking games.},
	urldate = {2021-04-20},
	journal = {arXiv:2102.07495 [cs]},
	author = {Shi, Naichen and Li, Ruichen and Youran, Sun},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.07495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Neural and Evolutionary Computing},
}

@article{lanctot_unified_2017,
	title = {A {Unified} {Game}-{Theoretic} {Approach} to {Multiagent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1711.00832},
	abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
	urldate = {2021-04-20},
	journal = {arXiv:1711.00832 [cs]},
	author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00832},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement learning: {An} introduction},
	publisher = {MIT press},
	author = {Sutton, Richard S and Barto, Andrew G},
	year = {2018},
}

@article{francois-lavet_introduction_2018,
	title = {An {Introduction} to {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1811.12560},
	doi = {10.1561/2200000071},
	abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
	urldate = {2021-04-19},
	journal = {arXiv:1811.12560 [cs, stat]},
	author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	month = dec,
	year = {2018},
	note = {arXiv: 1811.12560},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{konda_actor-critic_2000,
	title = {Actor-{Critic} {Algorithms}},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
	language = {en},
	journal = {Advances in neural information processing systems},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	year = {2000},
	pages = {7},
}

@article{konda_tsitsiklis_2003,
	title = {Tsitsiklis, “{On} actor-critic algorithms},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information pro-vided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems. 1},
	journal = {SIAM J. Control Optim},
	author = {Konda, Vijay R. and Tsitsiklis, John N.},
	year = {2003},
	pages = {1143--1166},
}

@article{timbers_approximate_2020,
	title = {Approximate exploitability: {Learning} a best response in large games},
	shorttitle = {Approximate exploitability},
	url = {http://arxiv.org/abs/2004.09677},
	abstract = {A standard metric used to measure the approximate optimality of policies in imperfect information games is exploitability, i.e. the performance of a policy against its worst-case opponent. However, exploitability is intractable to compute in large games as it requires a full traversal of the game tree to calculate a best response to the given policy. We introduce a new metric, approximate exploitability, that calculates an analogous metric using an approximate best response; the approximation is done by using search and reinforcement learning. This is a generalization of local best response, a domain specific evaluation metric used in poker. We provide empirical results for a specific instance of the method, demonstrating that our method converges to exploitability in the tabular and function approximation settings for small games. In large games, our method learns to exploit both strong and weak agents, learning to exploit an AlphaZero agent.},
	urldate = {2021-04-19},
	journal = {arXiv:2004.09677 [cs, stat]},
	author = {Timbers, Finbarr and Lockhart, Edward and Lanctot, Marc and Schmid, Martin and Schrittwieser, Julian and Hubert, Thomas and Bowling, Michael},
	month = sep,
	year = {2020},
	note = {arXiv: 2004.09677},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lillicrap_continuous_2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2021-04-16},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	urldate = {2021-04-16},
	journal = {arXiv:1801.01290 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.01290},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{schulman_trust_2017,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	urldate = {2021-04-16},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2021-04-16},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
}

@article{charlesworth_application_2018,
	title = {Application of {Self}-{Play} {Reinforcement} {Learning} to a {Four}-{Player} {Game} of {Imperfect} {Information}},
	url = {http://arxiv.org/abs/1808.10442},
	abstract = {We introduce a new virtual environment for simulating a card game known as "Big 2". This is a four-player game of imperfect information with a relatively complicated action space (being allowed to play 1,2,3,4 or 5 card combinations from an initial starting hand of 13 cards). As such it poses a challenge for many current reinforcement learning methods. We then use the recently proposed "Proximal Policy Optimization" algorithm to train a deep neural network to play the game, purely learning via self-play, and find that it is able to reach a level which outperforms amateur human players after only a relatively short amount of training time and without needing to search a tree of future game states.},
	urldate = {2021-04-15},
	journal = {arXiv:1808.10442 [cs, stat]},
	author = {Charlesworth, Henry},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10442},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-04-15},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
}

@article{silver_mastering_2017-1,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2021-04-15},
	journal = {arXiv:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01815},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{miller_tichu_nodate,
	title = {Tichu {Bot}},
	language = {en},
	author = {Miller, Peter},
	pages = {29},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2021-04-15},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	pages = {484--489},
}

@article{warchalski_deep_2020,
	title = {Deep {RL} {Agent} for a {Real}-{Time} {Action} {Strategy} {Game}},
	url = {http://arxiv.org/abs/2002.06290},
	abstract = {We introduce a reinforcement learning environment based on Heroic - Magic Duel, a 1 v 1 action strategy game. This domain is non-trivial for several reasons: it is a real-time game, the state space is large, the information given to the player before and at each step of a match is imperfect, and distribution of actions is dynamic. Our main contribution is a deep reinforcement learning agent playing the game at a competitive level that we trained using PPO and self-play with multiple competing agents, employing only a simple reward of \${\textbackslash}pm 1\$ depending on the outcome of a single match. Our best self-play agent, obtains around \$65{\textbackslash}\%\$ win rate against the existing AI and over \$50{\textbackslash}\%\$ win rate against a top human player.},
	urldate = {2021-04-07},
	journal = {arXiv:2002.06290 [cs]},
	author = {Warchalski, Michal and Radojevic, Dimitrije and Milosevic, Milos},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06290
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{avarikioti_distributed_nodate,
	title = {Distributed {Computing} {Group} {Computer} {Engineering} and {Networks} {Laboratory} {ETH} {Zürich}},
	language = {en},
	author = {Avarikioti, Zeta and Faber, Lukas and Wattenhofer, Dr Roger},
	pages = {29},
}

@inproceedings{niklaus_survey_2019,
	title = {Survey of {Artificial} {Intelligence} for {Card} {Games} and {Its} {Application} to the {Swiss} {Game} {Jass}},
	doi = {10.1109/SDS.2019.00-12},
	abstract = {In the last decades we have witnessed the success of applications of Artificial Intelligence to playing games. In this work we address the challenging field of games with hidden information and card games in particular. Jass is a very popular card game in Switzerland and is closely connected with Swiss culture. To the best of our knowledge, performances of Artificial Intelligence agents in the game of Jass do not outperform top players yet. Our contribution to the community is two-fold. First, we provide an overview of the current state-of-the-art of Artificial Intelligence methods for card games in general. Second, we discuss their application to the use-case of the Swiss card game Jass. This paper aims to be an entry point for both seasoned researchers and new practitioners who want to join in the Jass challenge.},
	booktitle = {2019 6th {Swiss} {Conference} on {Data} {Science} ({SDS})},
	author = {Niklaus, J. and Alberti, M. and Pondenkandath, V. and Ingold, R. and Liwicki, M.},
	month = jun,
	year = {2019},
	keywords = {Artificial Intelligence, Card Games, Convergence, Games, Hidden Information, Jass, Machine learning, Minimization, Monte Carlo methods, Numerical models, Reinforcement Learning},
	pages = {25--30},
}

@article{parker-holder_provably_2021,
	title = {Provably {Efficient} {Online} {Hyperparameter} {Optimization} with {Population}-{Based} {Bandits}},
	url = {http://arxiv.org/abs/2002.02518},
	abstract = {Many of the recent triumphs in machine learning are dependent on well-tuned hyperparameters. This is particularly prominent in reinforcement learning (RL) where a small change in the configuration can lead to failure. Despite the importance of tuning hyperparameters, it remains expensive and is often done in a naive and laborious way. A recent solution to this problem is Population Based Training (PBT) which updates both weights and hyperparameters in a single training run of a population of agents. PBT has been shown to be particularly effective in RL, leading to widespread use in the field. However, PBT lacks theoretical guarantees since it relies on random heuristics to explore the hyperparameter space. This inefficiency means it typically requires vast computational resources, which is prohibitive for many small and medium sized labs. In this work, we introduce the first provably efficient PBT-style algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to guide the search in an efficient way, making it possible to discover high performing hyperparameter configurations with far fewer agents than typically required by PBT. We show in a series of RL experiments that PB2 is able to achieve high performance with a modest computational budget.},
	urldate = {2021-04-03},
	journal = {arXiv:2002.02518 [cs, stat]},
	author = {Parker-Holder, Jack and Nguyen, Vu and Roberts, Stephen},
	month = feb,
	year = {2021},
	note = {arXiv: 2002.02518},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hernandez_metagame_2020,
	title = {Metagame {Autobalancing} for {Competitive} {Multiplayer} {Games}},
	doi = {10.1109/CoG47356.2020.9231762},
	abstract = {Automated game balancing has often focused on single-agent scenarios. In this paper we present a tool for balancing multi-player games during game design. Our approach requires a designer to construct an intuitive graphical representation of their meta-game target, representing the relative scores that high-level strategies (or decks, or character types) should experience. This permits more sophisticated balance targets to be defined beyond a simple requirement of equal win chances. We then find a parameterization of the game that meets this target using simulation-based optimization to minimize the distance to the target graph. We show the capabilities of this tool on examples inheriting from Rock-Paper-Scissors, and on a more complex asymmetric fighting game.},
	booktitle = {2020 {IEEE} {Conference} on {Games} ({CoG})},
	author = {Hernandez, D. and Gbadamosi, C. T. Toyin and Goodman, J. and Walker, J. A.},
	month = aug,
	year = {2020},
	note = {ISSN: 2325-4289},
	keywords = {Cognition, Games, Reinforcement learning, Sociology, Statistics, Testing, Tools},
	pages = {275--282},
}

@article{jaderberg_human-level_2019,
	title = {Human-level performance in first-person multiplayer games with population-based deep reinforcement learning},
	volume = {364},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/1807.01281},
	doi = {10.1126/science.aau6249},
	abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
	number = {6443},
	urldate = {2021-03-30},
	journal = {Science},
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	month = may,
	year = {2019},
	note = {arXiv: 1807.01281},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {859--865},
}

@article{bansal_emergent_2018,
	title = {Emergent {Complexity} via {Multi}-{Agent} {Competition}},
	url = {http://arxiv.org/abs/1710.03748},
	abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
	urldate = {2021-03-29},
	journal = {arXiv:1710.03748 [cs]},
	author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
	month = mar,
	year = {2018},
	note = {arXiv: 1710.03748},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{seita_offline_nodate,
	title = {Offline {Reinforcement} {Learning}: {How} {Conservative} {Algorithms} {Can} {Enable} {New} {Applications}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://bair.berkeley.edu/blog/2020/12/07/offline/},
	abstract = {The BAIR Blog},
	urldate = {2021-03-28},
	journal = {The Berkeley Artificial Intelligence Research Blog},
	author = {Seita, Daniel},
}

@article{hernandez_comparison_2020,
	title = {A {Comparison} of {Self}-{Play} {Algorithms} {Under} a {Generalized} {Framework}},
	url = {http://arxiv.org/abs/2006.04471},
	abstract = {Throughout scientific history, overarching theoretical frameworks have allowed researchers to grow beyond personal intuitions and culturally biased theories. They allow to verify and replicate existing findings, and to link is connected results. The notion of self-play, albeit often cited in multiagent Reinforcement Learning, has never been grounded in a formal model. We present a formalized framework, with clearly defined assumptions, which encapsulates the meaning of self-play as abstracted from various existing self-play algorithms. This framework is framed as an approximation to a theoretical solution concept for multiagent training. On a simple environment, we qualitatively measure how well a subset of the captured self-play methods approximate this solution when paired with the famous PPO algorithm. We also provide insights on interpreting quantitative metrics of performance for self-play training. Our results indicate that, throughout training, various self-play definitions exhibit cyclic policy evolutions.},
	urldate = {2021-03-27},
	journal = {arXiv:2006.04471 [cs]},
	author = {Hernandez, Daniel and Denamganai, Kevin and Devlin, Sam and Samothrakis, Spyridon and Walker, James Alfred},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04471},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
}

@misc{seita_scaling_nodate,
	title = {Scaling {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://bair.berkeley.edu/blog/2018/12/12/rllib/},
	abstract = {The BAIR Blog},
	urldate = {2021-03-25},
	journal = {The Berkeley Artificial Intelligence Research Blog},
	author = {Seita, Daniel},
}

@article{jordan_evaluating_nodate,
	title = {Evaluating the {Performance} of {Reinforcement} {Learning} {Algorithms}},
	abstract = {Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difﬁcult to replicate. In this work, we argue that the inconsistency of performance stems from the use of ﬂawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks.},
	language = {en},
	author = {Jordan, Scott M and Chandak, Yash and Cohen, Daniel and Zhang, Mengxue and Thomas, Philip S},
	pages = {30},
}

@article{hessel_rainbow_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2021-03-22},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{shi_scrofazero_2021-1,
	title = {{ScrofaZero}: {Mastering} {Trick}-taking {Poker} {Game} {Gongzhu} by {Deep} {Reinforcement} {Learning}},
	shorttitle = {{ScrofaZero}},
	url = {http://arxiv.org/abs/2102.07495},
	abstract = {People have made remarkable progress in game AIs, especially in domain of perfect information game. However, trick-taking poker game, as a popular form of imperfect information game, has been regarded as a challenge for a long time. Since trick-taking game requires high level of not only reasoning, but also inference to excel, it can be a new milestone for imperfect information game AI. We study Gongzhu, a trick-taking game analogous to, but slightly simpler than contract bridge. Nonetheless, the strategies of Gongzhu are complex enough for both human and computer players. We train a strong Gongzhu AI ScrofaZero from {\textbackslash}textit\{tabula rasa\} by deep reinforcement learning, while few previous efforts on solving trick-taking poker game utilize the representation power of neural networks. Also, we introduce new techniques for imperfect information game including stratified sampling, importance weighting, integral over equivalent class, Bayesian inference, etc. Our AI can achieve human expert level performance. The methodologies in building our program can be easily transferred into a wide range of trick-taking games.},
	urldate = {2021-03-02},
	journal = {arXiv:2102.07495 [cs]},
	author = {Shi, Naichen and Li, Ruichen and Youran, Sun},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.07495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Neural and Evolutionary Computing},
}

@article{brown_combining_2020,
	title = {Combining {Deep} {Reinforcement} {Learning} and {Search} for {Imperfect}-{Information} {Games}},
	url = {http://arxiv.org/abs/2007.13544},
	abstract = {The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.},
	urldate = {2021-02-26},
	journal = {arXiv:2007.13544 [cs]},
	author = {Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
	month = nov,
	year = {2020},
	note = {arXiv: 2007.13544},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-02-22},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2021-02-22},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
}

@article{brown_deep_2019,
	title = {Deep {Counterfactual} {Regret} {Minimization}},
	url = {http://arxiv.org/abs/1811.00164},
	abstract = {Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.},
	urldate = {2021-02-17},
	journal = {arXiv:1811.00164 [cs]},
	author = {Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
	month = may,
	year = {2019},
	note = {arXiv: 1811.00164},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	number = {3-4},
	journal = {Machine learning},
	author = {Watkins, Christopher JCH and Dayan, Peter},
	year = {1992},
	note = {Publisher: Springer},
	pages = {279--292},
}

@article{schulman_proximal_2017-1,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2021-01-09},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2021-01-09},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning},
}

@article{van_hasselt_deep_2015-1,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-01-09},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
}

@article{hernandez-leal_survey_2019,
	title = {A {Survey} and {Critique} of {Multiagent} {Deep} {Reinforcement} {Learning}},
	volume = {33},
	issn = {1387-2532, 1573-7454},
	url = {http://arxiv.org/abs/1810.05587},
	doi = {10.1007/s10458-019-09421-1},
	abstract = {Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.},
	number = {6},
	urldate = {2021-01-09},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
	month = nov,
	year = {2019},
	note = {arXiv: 1810.05587},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	pages = {750--797},
}

@misc{noauthor_demystifying_nodate,
	title = {Demystifying {Deep} {Reinforcement} {Learning} {\textbar} {Computational} {Neuroscience} {Lab}},
	url = {https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/},
	language = {en-US},
	urldate = {2021-01-08},
}

@article{lanctot_openspiel_2020,
	title = {{OpenSpiel}: {A} {Framework} for {Reinforcement} {Learning} in {Games}},
	shorttitle = {{OpenSpiel}},
	url = {http://arxiv.org/abs/1908.09453},
	abstract = {OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games. OpenSpiel supports n-player (single- and multi- agent) zero-sum, cooperative and general-sum, one-shot and sequential, strictly turn-taking and simultaneous-move, perfect and imperfect information games, as well as traditional multiagent environments such as (partially- and fully- observable) grid worlds and social dilemmas. OpenSpiel also includes tools to analyze learning dynamics and other common evaluation metrics. This document serves both as an overview of the code base and an introduction to the terminology, core concepts, and algorithms across the fields of reinforcement learning, computational game theory, and search.},
	urldate = {2021-01-08},
	journal = {arXiv:1908.09453 [cs]},
	author = {Lanctot, Marc and Lockhart, Edward and Lespiau, Jean-Baptiste and Zambaldi, Vinicius and Upadhyay, Satyaki and Pérolat, Julien and Srinivasan, Sriram and Timbers, Finbarr and Tuyls, Karl and Omidshafiei, Shayegan and Hennes, Daniel and Morrill, Dustin and Muller, Paul and Ewalds, Timo and Faulkner, Ryan and Kramár, János and De Vylder, Bart and Saeta, Brennan and Bradbury, James and Ding, David and Borgeaud, Sebastian and Lai, Matthew and Schrittwieser, Julian and Anthony, Thomas and Hughes, Edward and Danihelka, Ivo and Ryan-Davis, Jonah},
	month = sep,
	year = {2020},
	note = {arXiv: 1908.09453},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{tomasev_assessing_2020,
	title = {Assessing {Game} {Balance} with {AlphaZero}: {Exploring} {Alternative} {Rule} {Sets} in {Chess}},
	shorttitle = {Assessing {Game} {Balance} with {AlphaZero}},
	url = {http://arxiv.org/abs/2009.04374},
	abstract = {It is non-trivial to design engaging and balanced sets of game rules. Modern chess has evolved over centuries, but without a similar recourse to history, the consequences of rule changes to game dynamics are difficult to predict. AlphaZero provides an alternative in silico means of game balance assessment. It is a system that can learn near-optimal strategies for any rule set from scratch, without any human supervision, by continually learning from its own experience. In this study we use AlphaZero to creatively explore and design new chess variants. There is growing interest in chess variants like Fischer Random Chess, because of classical chess's voluminous opening theory, the high percentage of draws in professional play, and the non-negligible number of games that end while both players are still in their home preparation. We compare nine other variants that involve atomic changes to the rules of chess. The changes allow for novel strategic and tactical patterns to emerge, while keeping the games close to the original. By learning near-optimal strategies for each variant with AlphaZero, we determine what games between strong human players might look like if these variants were adopted. Qualitatively, several variants are very dynamic. An analytic comparison show that pieces are valued differently between variants, and that some variants are more decisive than classical chess. Our findings demonstrate the rich possibilities that lie beyond the rules of modern chess.},
	urldate = {2021-01-08},
	journal = {arXiv:2009.04374 [cs, stat]},
	author = {Tomašev, Nenad and Paquet, Ulrich and Hassabis, Demis and Kramnik, Vladimir},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.04374},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
}

@misc{noauthor_heads-up_nodate,
	title = {Heads-up limit hold’em poker is solved {\textbar} {Science}},
	url = {https://science.sciencemag.org/content/347/6218/145.full},
	urldate = {2021-01-08},
}

@inproceedings{brown_libratus_2017,
	address = {Melbourne, Australia},
	title = {Libratus: {The} {Superhuman} {AI} for {No}-{Limit} {Poker}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {Libratus},
	url = {https://www.ijcai.org/proceedings/2017/772},
	doi = {10.24963/ijcai.2017/772},
	abstract = {No-limit Texas Hold’em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold’em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the ﬁrst—and so far only—AI to defeat top human professionals in that game. Libratus’s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.},
	language = {en},
	urldate = {2021-01-05},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Brown, Noam and Sandholm, Tuomas},
	month = aug,
	year = {2017},
	pages = {5226--5228},
}

@article{brown_superhuman_2018,
	title = {Superhuman {AI} for heads-up no-limit poker: {Libratus} beats top professionals},
	volume = {359},
	copyright = {Copyright © 2018, The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Superhuman {AI} for heads-up no-limit poker},
	url = {https://science.sciencemag.org/content/359/6374/418},
	doi = {10.1126/science.aao1733},
	abstract = {Libratus versus humans
Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold'em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent.
Science, this issue p. 418
No-limit Texas hold’em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold’em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.
An artificial intelligence program called Libratus played 120,000 hands of a two-player variant of poker and beat four leading human professionals.
An artificial intelligence program called Libratus played 120,000 hands of a two-player variant of poker and beat four leading human professionals.},
	language = {en},
	number = {6374},
	urldate = {2021-01-05},
	journal = {Science},
	author = {Brown, Noam and Sandholm, Tuomas},
	month = jan,
	year = {2018},
	pmid = {29249696},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	pages = {418--424},
}

@article{zhang_multi-agent_2019,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {A} {Selective} {Overview} of {Theories} and {Algorithms}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1911.10635},
	abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
	urldate = {2021-01-05},
	journal = {arXiv:1911.10635 [cs, stat]},
	author = {Zhang, Kaiqing and Yang, Zhuoran and Başar, Tamer},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.10635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2021-01-05},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
}

@article{nguyen_deep_2019,
	title = {Deep {Reinforcement} {Learning} for {Multi}-{Agent} {Systems}: {A} {Review} of {Challenges}, {Solutions} and {Applications}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Multi}-{Agent} {Systems}},
	url = {http://arxiv.org/abs/1812.11794},
	doi = {10.1109/TCYB.2020.2977374},
	abstract = {Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms however have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This paper addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multi-agent deep RL (MADRL) is presented, including non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, multi-agent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed, with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to future development of more robust and highly useful multi-agent learning methods for solving real-world problems.},
	urldate = {2021-01-02},
	journal = {arXiv:1812.11794 [cs, stat]},
	author = {Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
	month = feb,
	year = {2019},
	note = {arXiv: 1812.11794},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{heinrich_deep_2016,
	title = {Deep {Reinforcement} {Learning} from {Self}-{Play} in {Imperfect}-{Information} {Games}},
	url = {http://arxiv.org/abs/1603.01121},
	abstract = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.},
	urldate = {2021-01-02},
	journal = {arXiv:1603.01121 [cs]},
	author = {Heinrich, Johannes and Silver, David},
	month = jun,
	year = {2016},
	note = {arXiv: 1603.01121},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@article{barros_learning_2020,
	title = {Learning from {Learners}: {Adapting} {Reinforcement} {Learning} {Agents} to be {Competitive} in a {Card} {Game}},
	shorttitle = {Learning from {Learners}},
	url = {http://arxiv.org/abs/2004.04000},
	abstract = {Learning how to adapt to complex and dynamic environments is one of the most important factors that contribute to our intelligence. Endowing artificial agents with this ability is not a simple task, particularly in competitive scenarios. In this paper, we present a broad study on how popular reinforcement learning algorithms can be adapted and implemented to learn and to play a real-world implementation of a competitive multiplayer card game. We propose specific training and validation routines for the learning agents, in order to evaluate how the agents learn to be competitive and explain how they adapt to each others' playing style. Finally, we pinpoint how the behavior of each agent derives from their learning style and create a baseline for future research on this scenario.},
	urldate = {2021-01-02},
	journal = {arXiv:2004.04000 [cs]},
	author = {Barros, Pablo and Tanevska, Ana and Sciutti, Alessandra},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.04000},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{jiang_deltadou_2019-1,
	address = {Macao, China},
	title = {{DeltaDou}: {Expert}-level {Doudizhu} {AI} through {Self}-play},
	isbn = {978-0-9992411-4-1},
	shorttitle = {{DeltaDou}},
	url = {https://www.ijcai.org/proceedings/2019/176},
	doi = {10.24963/ijcai.2019/176},
	abstract = {Artiﬁcial Intelligence has seen several breakthroughs in two-player perfect information game. Nevertheless, Doudizhu, a three-player imperfect information game, is still quite challenging. In this paper, we present a Doudizhu AI by applying deep reinforcement learning from games of self-play. The algorithm combines an asymmetric MCTS on nodes representing each player’s information set, a policy-value network that approximates the policy and value on each decision node, and inference on unobserved hands of other players by given policy. Our results show that self-play can signiﬁcantly improve the performance of our agent in this multiagent imperfect information game. Even starting with a weak AI, our agent can achieve human expert level after days of self-play and training.},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Jiang, Qiqi and Li, Kuangzheng and Du, Boyao and Chen, Hao and Fang, Hai},
	month = aug,
	year = {2019},
	pages = {1265--1271},
}

@misc{noauthor_rules_nodate,
	title = {Rules of {Card} {Games}: {Yaniv}},
	url = {https://www.pagat.com/draw/yaniv.html},
	urldate = {2020-12-29},
}

@article{zha_rlcard_2020,
	title = {{RLCard}: {A} {Toolkit} for {Reinforcement} {Learning} in {Card} {Games}},
	shorttitle = {{RLCard}},
	url = {http://arxiv.org/abs/1910.04376},
	abstract = {RLCard is an open-source toolkit for reinforcement learning research in card games. It supports various card environments with easy-to-use interfaces, including Blackjack, Leduc Hold'em, Texas Hold'em, UNO, Dou Dizhu and Mahjong. The goal of RLCard is to bridge reinforcement learning and imperfect information games, and push forward the research of reinforcement learning in domains with multiple agents, large state and action space, and sparse reward. In this paper, we provide an overview of the key components in RLCard, a discussion of the design principles, a brief introduction of the interfaces, and comprehensive evaluations of the environments. The codes and documents are available at https://github.com/datamllab/rlcard},
	urldate = {2020-12-29},
	journal = {arXiv:1910.04376 [cs]},
	author = {Zha, Daochen and Lai, Kwei-Herng and Cao, Yuanpu and Huang, Songyi and Wei, Ruzhe and Guo, Junyu and Hu, Xia},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.04376},
	keywords = {Computer Science - Artificial Intelligence},
}
