\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Experiment Design} \label{method:experiments}
\note{need to double check that these are actually normally distributed!!}
\begin{itemize}[nosep]
    \item \note{medium instead of mode}
    \item \note{box and whisker plots}
    \item \note{mann whitney u test for significance}
\end{itemize}

\note{Include text explaining how the win-rate matrix works and how the results will be analysed (check the learning from learners paper)}

The ideas of luck and skill are difficult to quantify together. One way to try and eliminate the effects of chance is to run simulations over many thousands of games \cite{guo_distinguishing_2019}. Allowing us to gain a truer measure of skill. 

In order to see the effects luck has on the game it is possible to synthesise events which have a positive impact on the player but may be unlikely to occur. To investigate this idea I propose two sets of experiments, the first looks at the effect of starting a game, and the second at the effect of a player's starting hand.

\note{mention the number of games needed for significance}

All of these experiments involve simulating games with the environment and agents developed in \cref{method:implementation}. In each experiment one of the events mentioned in \cref{intro:reserach-qs} is fixed to simulate luck, while all other parameters of the environment remain standard. 

\subsection{The effect of going first} \label{method:going-first}
In order to test \cref{hyp:going-first} -- the effect going first has on a player's win rate -- I used the rule based agents developed in \autoref{method:rule_agents} and the same environment used to generate \autoref{tab:rules-winrates} but with the starting player fixed over the tournament.

For each combination of the heuristic agents, set up a tournament to simulate games. Fix the starting player to the first agent, simulate 10000 games, then fix the starting player to the second agent and simulate another 10000 games. For the case when the agent plays itself, it is only necessary to simulate one set of games with the starting player fixed. The win-rate of the starting player is then recorded. The results are shown in \autoref{tab:rule-wr-going-first}, along with the increases in performance over \autoref{tab:rules-winrates} shown in brackets.

\note{Need to repeat \autoref{tab:rule-wr-going-first} and \ref{tab:rules-winrates} with significantly more data points. Should be able to parallelize it with ray}

In order to test \cref{hyp:first-overtime} I use the RL model's saved policies as a measure of increasing skill. Every 10th training iteration the RL model saves the current version of its policy. Using these policy snapshots its able to run experiments over the RL agents training life, which simulates the idea of getting better. \autoref{fig:model_vs_rules} clearly shows the increase in skill over training iteration against the rule based agents. 

At each saved model 10,000 games are simulated with player 0 going first each time. The win rate of player 0 at each model is plotted on \autoref{fig:win-rate-going-first}.


\subsection{The effect of starting hands}
\begin{table}[]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Class & Description                           & Example        & Frequency \\ \midrule
5S    & 5 card straight                       & C2 C3 C4 C5 C6 & 36        \\
4K    & 4 of a kind                           & C2 H2 S2 D2 C7 & 624       \\
4S    & 4 card straight                       & C2 C3 C4 C5 D4 & 1848      \\
3S2K  & 3 card straight and a pair            & C2 C3 C4 D2 D2 & 2796      \\
3K2K  & Full house                            & C2 D2 H2 S4 H4 & 3744      \\
3S    & 3 card straight                       & C2 C3 C4 H5 S3 & 45144     \\
3K    & 3 of a kind                           & C2 D2 H2 S6 H5 & 54516     \\
2K2K  & Two pair                              & C2 D2 H4 S4 H7 & 2376      \\
2K    & Pair                                  & D7 C7 H2 D6 H3 & 1202604   \\
X     & High Card                             & D7 S9 H2 C5 D3 & 1287648   \\ \bottomrule
\end{tabular}
\caption{Yaniv hand classes.}
\label{tab:hand-classes}
\end{table}

In order to test \cref{hyp:hand-score} -- the effect of starting a round with a hand of a specific score -- I have simulated games where the starting hand of one player is forced to be of a certain score. I used one RL agent to make decisions for both players. \numprint{20000} games were simulated per starting hand score. Results in \autoref{fig:startinghand-winrates}.

In order to \cref{hyp:hand-class} I used both the intermediate agent and RL Model. The first thing to do is determine the influence starting hand has during self-play. This eliminates the difference in skill between two agents. For each hand class simulate a number of games where the starting hand of player 0 is sampled from the current hand class, while player 1's hand is sampled randomly from the remaining deck. The results of this can be seen in \autoref{fig:handclass-vs-self}. 

Next it is useful to do the same experiment but with different opponents. \autoref{fig:handclass-int-vs-model} shows the results of the intermediate model vs the 10,000th iteration of the RL model. The experiment is conducted in the same way but for each hand class both the agents get a turn at sampling from the class. The aim of this is to show whether an agent can become more skilled at defending against certain unfair advantages. 


To test \cref{hyp:class-overtime} I use a similar experiment to \cref{hyp:hand-class} and \cref{hyp:first-overtime}. For each model and starting hand class run a series of game simulations where the model is used as the policy for both players. The starting player is chosen randomly per game, and the opponents hand is sampled from the deck after a hand is sampled from the hand class. Results in \autoref{fig:handclass-overtime}.

\end{document}